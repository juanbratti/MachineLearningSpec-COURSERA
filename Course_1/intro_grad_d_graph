import numpy as np
import math, copy
import matplotlib.pyplot as plt
plt.style.use('./deeplearning.mplstyle')
from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients

# Load our data set
x_train = np.array([1.0, 2.0])   #features
y_train = np.array([300.0, 500.0])   #target value

def compute_cost(x : np.ndarray, y : np.ndarray, w : float, b : float) -> float:
    """
    Computes the cost function for linear regression.

    Args:
        x (ndarray (m,)): Data, m examples
        y (ndarray (m,)): target values
        w,b (scalar)    : model parameters

    Returns:
        total_cost (float): The cost of using w,b as the parameters 
        for linear regression to fit the data points in x and y
    """

    # amount of values
    m = len(x)

    # calculating predictions
    predictions = w*x + b #f_wb
    total_cost = (np.sum((predictions - y)**2))/(2*m)

    return total_cost

def compute_derivatives(x:np.ndarray, y:np.ndarray, w:float ,b:float) -> float:

    """
    Computes the derivatives part of the gradient descent algorithm

    Args:
        x (ndarray (m,)): Data, m examples
        y (ndarray (m,)): target values
        w,b (scalar)    : model parameters

    Returns:
        dj_dw
        dj_b
    """
    
    dj_dw = 0
    dj_db = 0

    m = x.shape[0]

    dj_dw = np.sum(((w*x + b)-y)*x)/m
    dj_db = np.sum((w*x + b)-y)/m

    return dj_dw, dj_db


def gradient_descent(x : np.ndarray, y : np.ndarray, w_in : float, b_in : float, alpha : float, num_iters : int, cost_function, gradient_function):
    """
    Performs gradient descent to fit w,b. Updates w,b by taking num_iters gradient
    with learning rate alpha

    Args:
        x (ndarray (m,)) : Data, m examples
        y (ndarray (m,)) : target values
        w_in, b_in (scalar) : initial values of model parameters
        alpha (float) : learning rate
        num_iters (int) : number of iteration to run gradient descent
        cost_function: function to call to produce cost
        gradient_function: function to call to produce gradient

    Returns:
        w (scalar) : updated value of parameter after running gradient descent
        b (scalar) : updated value of parameter after running gradient descent
        J_history (list): history of cost values
        p_history (list): History of parameters [w,b]
    """

    J_history = []
    p_history = []
    b = b_in
    w = w_in

    for i in range(num_iters):
        dj_dw, dj_db = compute_derivatives(x, y, w, b)
        b = b - alpha * dj_db
        w = w - alpha * dj_dw
        
        if i < 100000:
            J_history.append(cost_function(x,y,w,b))
            p_history.append([w,b])

        if i% math.ceil(num_iters/10) == 0:
            print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w: 0.3e}, b:{b: 0.5e}")


    return w, b, J_history, p_history #return w and J,w history for graphing


# initialize parameters
w_init = 0
b_init = 0
# some gradient descent settings
iterations = 10000
tmp_alpha = 1.0e-2
# run gradient descent
w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, 
                                                    iterations, compute_cost, compute_derivatives)
print(f"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})")